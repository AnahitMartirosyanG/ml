{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58560a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d060c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a345f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hospital_deaths_train.csv')\n",
    "df= data.drop(['recordid'], axis = 1)\n",
    "y = data['In-hospital_death']\n",
    "df= df.drop(['In-hospital_death'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046759e",
   "metadata": {},
   "source": [
    "### Prepocessing from groupwork +one hot encoding, normalizing by me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbf27c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3250, 87), (3250,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_nans = []                                                   \n",
    "for col in df.columns:\n",
    "    if (df[col].isna().sum() / len(df)) * 100 > 40:               \n",
    "         many_nans.append(col)\n",
    "df = df.drop(columns=many_nans, axis=1)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "dropped_cols = set(df.columns) - set(df.columns)       \n",
    "#one hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc.fit(df[['Gender']])\n",
    "data1 = pd.DataFrame(enc.transform(df[['Gender']]))\n",
    "data = pd.concat([df, data1], axis = 1)\n",
    "data = data.drop(['Gender'], axis = 1)\n",
    "#normalizing\n",
    "norm = MinMaxScaler().fit(data)\n",
    "data = norm.transform(data)\n",
    "X = pd.DataFrame(data)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbeb20b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2925, 87), (325, 87), (2925,), (325,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=11)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c97b135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anahit\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 3s 3ms/step - loss: 0.3614 - accuracy: 0.8598 - precision: 0.5625 - recall: 0.0435\n",
      "Epoch 2/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8656 - precision: 0.6040 - recall: 0.1473 \n",
      "Epoch 3/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8615 - precision: 0.5398 - recall: 0.1473\n",
      "Epoch 4/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8680 - precision: 0.6944 - recall: 0.1208  \n",
      "Epoch 5/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8615 - precision: 0.7368 - recall: 0.0338   \n",
      "Epoch 6/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.8728 - precision: 0.6438 - recall: 0.2271 \n",
      "Epoch 7/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.8691 - precision: 0.6303 - recall: 0.1812 \n",
      "Epoch 8/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8694 - precision: 0.5851 - recall: 0.2657\n",
      "Epoch 9/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3131 - accuracy: 0.8721 - precision: 0.6149 - recall: 0.2585 \n",
      "Epoch 10/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8732 - precision: 0.6352 - recall: 0.2440\n",
      "Epoch 11/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3128 - accuracy: 0.8684 - precision: 0.6142 - recall: 0.1884 \n",
      "Epoch 12/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8718 - precision: 0.6242 - recall: 0.2367 \n",
      "Epoch 13/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8735 - precision: 0.6507 - recall: 0.2295\n",
      "Epoch 14/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8670 - precision: 0.5767 - recall: 0.2271\n",
      "Epoch 15/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.8721 - precision: 0.6724 - recall: 0.1884 \n",
      "Epoch 16/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.8728 - precision: 0.6522 - recall: 0.2174\n",
      "Epoch 17/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.8749 - precision: 0.6333 - recall: 0.2754 \n",
      "Epoch 18/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8708 - precision: 0.6268 - recall: 0.2150\n",
      "Epoch 19/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8779 - precision: 0.6686 - recall: 0.2729\n",
      "Epoch 20/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8779 - precision: 0.6592 - recall: 0.2850\n",
      "Epoch 21/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8766 - precision: 0.6432 - recall: 0.2874\n",
      "Epoch 22/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8697 - precision: 0.6279 - recall: 0.1957\n",
      "Epoch 23/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2983 - accuracy: 0.8790 - precision: 0.6875 - recall: 0.2657\n",
      "Epoch 24/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8790 - precision: 0.6630 - recall: 0.2947\n",
      "Epoch 25/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8756 - precision: 0.6623 - recall: 0.2464\n",
      "Epoch 26/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8735 - precision: 0.6392 - recall: 0.2440\n",
      "Epoch 27/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8779 - precision: 0.6557 - recall: 0.2899\n",
      "Epoch 28/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8786 - precision: 0.6630 - recall: 0.2899\n",
      "Epoch 29/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8769 - precision: 0.7177 - recall: 0.2150\n",
      "Epoch 30/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8759 - precision: 0.6349 - recall: 0.2899\n",
      "Epoch 31/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8827 - precision: 0.7052 - recall: 0.2947\n",
      "Epoch 32/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8827 - precision: 0.6919 - recall: 0.3092\n",
      "Epoch 33/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8827 - precision: 0.7178 - recall: 0.2826\n",
      "Epoch 34/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8814 - precision: 0.6872 - recall: 0.2971\n",
      "Epoch 35/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2947 - accuracy: 0.8786 - precision: 0.7122 - recall: 0.2391\n",
      "Epoch 36/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8783 - precision: 0.6648 - recall: 0.2826\n",
      "Epoch 37/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8793 - precision: 0.6631 - recall: 0.2995\n",
      "Epoch 38/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8715 - precision: 0.6145 - recall: 0.2464\n",
      "Epoch 39/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8817 - precision: 0.6977 - recall: 0.2899\n",
      "Epoch 40/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8803 - precision: 0.6702 - recall: 0.3043\n",
      "Epoch 41/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8762 - precision: 0.6688 - recall: 0.2488\n",
      "Epoch 42/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8786 - precision: 0.6879 - recall: 0.2609\n",
      "Epoch 43/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2884 - accuracy: 0.8838 - precision: 0.7256 - recall: 0.2874\n",
      "Epoch 44/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.8756 - precision: 0.6453 - recall: 0.2681\n",
      "Epoch 45/300\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.2883 - accuracy: 0.8766 - precision: 0.6480 - recall: 0.2802\n",
      "Epoch 46/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2825 - accuracy: 0.8800 - precision: 0.7086 - recall: 0.2585\n",
      "Epoch 47/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8827 - precision: 0.7101 - recall: 0.2899\n",
      "Epoch 48/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8773 - precision: 0.6774 - recall: 0.2536\n",
      "Epoch 49/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8831 - precision: 0.6978 - recall: 0.3068\n",
      "Epoch 50/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8749 - precision: 0.6558 - recall: 0.2440\n",
      "Epoch 51/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.8817 - precision: 0.7000 - recall: 0.2874 \n",
      "Epoch 52/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.8858 - precision: 0.7083 - recall: 0.3285\n",
      "Epoch 53/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.8800 - precision: 0.6800 - recall: 0.2874\n",
      "Epoch 54/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.8827 - precision: 0.6961 - recall: 0.3043 \n",
      "Epoch 55/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8790 - precision: 0.6807 - recall: 0.2729\n",
      "Epoch 56/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8814 - precision: 0.6754 - recall: 0.3116\n",
      "Epoch 57/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.8773 - precision: 0.6571 - recall: 0.2778 \n",
      "Epoch 58/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.8803 - precision: 0.7462 - recall: 0.2343\n",
      "Epoch 59/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8851 - precision: 0.7321 - recall: 0.2971\n",
      "Epoch 60/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.8838 - precision: 0.6968 - recall: 0.3164\n",
      "Epoch 61/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8793 - precision: 0.7259 - recall: 0.2367\n",
      "Epoch 62/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2770 - accuracy: 0.8841 - precision: 0.7143 - recall: 0.3019\n",
      "Epoch 63/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8855 - precision: 0.7283 - recall: 0.3043\n",
      "Epoch 64/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.8855 - precision: 0.7453 - recall: 0.2899\n",
      "Epoch 65/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8844 - precision: 0.7468 - recall: 0.2778\n",
      "Epoch 66/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2780 - accuracy: 0.8855 - precision: 0.7135 - recall: 0.3188\n",
      "Epoch 67/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8834 - precision: 0.6746 - recall: 0.3406\n",
      "Epoch 68/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8803 - precision: 0.6928 - recall: 0.2778\n",
      "Epoch 69/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.8868 - precision: 0.7107 - recall: 0.3382\n",
      "Epoch 70/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.8831 - precision: 0.7169 - recall: 0.2874\n",
      "Epoch 71/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.8844 - precision: 0.7209 - recall: 0.2995\n",
      "Epoch 72/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8875 - precision: 0.7297 - recall: 0.3261 \n",
      "Epoch 73/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8848 - precision: 0.7251 - recall: 0.2995\n",
      "Epoch 74/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.8841 - precision: 0.7193 - recall: 0.2971\n",
      "Epoch 75/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8855 - precision: 0.7182 - recall: 0.3140\n",
      "Epoch 76/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8868 - precision: 0.7065 - recall: 0.3430\n",
      "Epoch 77/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8848 - precision: 0.7278 - recall: 0.2971\n",
      "Epoch 78/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8844 - precision: 0.7111 - recall: 0.3092\n",
      "Epoch 79/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.8834 - precision: 0.7786 - recall: 0.2464 \n",
      "Epoch 80/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8865 - precision: 0.7204 - recall: 0.3237\n",
      "Epoch 81/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8903 - precision: 0.7719 - recall: 0.3188\n",
      "Epoch 82/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8783 - precision: 0.6611 - recall: 0.2874\n",
      "Epoch 83/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.8855 - precision: 0.7158 - recall: 0.3164\n",
      "Epoch 84/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8858 - precision: 0.7198 - recall: 0.3164\n",
      "Epoch 85/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.8817 - precision: 0.7361 - recall: 0.2560 \n",
      "Epoch 86/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8879 - precision: 0.7363 - recall: 0.3237\n",
      "Epoch 87/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8848 - precision: 0.7692 - recall: 0.2657\n",
      "Epoch 88/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.8862 - precision: 0.7396 - recall: 0.3019 \n",
      "Epoch 89/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8855 - precision: 0.7394 - recall: 0.2947\n",
      "Epoch 90/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.8909 - precision: 0.7540 - recall: 0.3406\n",
      "Epoch 91/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.8882 - precision: 0.7458 - recall: 0.3188\n",
      "Epoch 92/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8868 - precision: 0.7485 - recall: 0.3019\n",
      "Epoch 93/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.8824 - precision: 0.7333 - recall: 0.2657\n",
      "Epoch 94/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8875 - precision: 0.7707 - recall: 0.2923\n",
      "Epoch 95/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.8841 - precision: 0.7301 - recall: 0.2874\n",
      "Epoch 96/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.8913 - precision: 0.7667 - recall: 0.3333\n",
      "Epoch 97/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8896 - precision: 0.7514 - recall: 0.3285\n",
      "Epoch 98/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.8838 - precision: 0.7176 - recall: 0.2947\n",
      "Epoch 99/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8896 - precision: 0.7600 - recall: 0.3213\n",
      "Epoch 100/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8892 - precision: 0.7500 - recall: 0.3261\n",
      "Epoch 101/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8838 - precision: 0.7467 - recall: 0.2705\n",
      "Epoch 102/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8872 - precision: 0.7838 - recall: 0.2802\n",
      "Epoch 103/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8851 - precision: 0.7868 - recall: 0.2585\n",
      "Epoch 104/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.8868 - precision: 0.7399 - recall: 0.3092\n",
      "Epoch 105/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8892 - precision: 0.7812 - recall: 0.3019\n",
      "Epoch 106/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.8824 - precision: 0.7273 - recall: 0.2705\n",
      "Epoch 107/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8844 - precision: 0.7603 - recall: 0.2681\n",
      "Epoch 108/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8848 - precision: 0.7391 - recall: 0.2874\n",
      "Epoch 109/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8872 - precision: 0.7692 - recall: 0.2899\n",
      "Epoch 110/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.8892 - precision: 0.7848 - recall: 0.2995\n",
      "Epoch 111/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8855 - precision: 0.7394 - recall: 0.2947\n",
      "Epoch 112/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8872 - precision: 0.7500 - recall: 0.3043\n",
      "Epoch 113/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8844 - precision: 0.7603 - recall: 0.2681\n",
      "Epoch 114/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8892 - precision: 0.7711 - recall: 0.3092\n",
      "Epoch 115/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.8933 - precision: 0.8072 - recall: 0.3237\n",
      "Epoch 116/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8875 - precision: 0.7202 - recall: 0.3357\n",
      "Epoch 117/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.8879 - precision: 0.7443 - recall: 0.3164\n",
      "Epoch 118/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8875 - precision: 0.7429 - recall: 0.3140\n",
      "Epoch 119/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.8923 - precision: 0.8037 - recall: 0.3164\n",
      "Epoch 120/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.8896 - precision: 0.7791 - recall: 0.3068\n",
      "Epoch 121/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.8868 - precision: 0.7578 - recall: 0.2947\n",
      "Epoch 122/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.8913 - precision: 0.7963 - recall: 0.3116\n",
      "Epoch 123/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2568 - accuracy: 0.8896 - precision: 0.7661 - recall: 0.3164\n",
      "Epoch 124/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.8892 - precision: 0.7647 - recall: 0.3140\n",
      "Epoch 125/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.8899 - precision: 0.7644 - recall: 0.3213 \n",
      "Epoch 126/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8868 - precision: 0.7785 - recall: 0.2802 \n",
      "Epoch 127/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8875 - precision: 0.7457 - recall: 0.3116\n",
      "Epoch 128/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.8916 - precision: 0.7870 - recall: 0.3213 \n",
      "Epoch 129/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8899 - precision: 0.8108 - recall: 0.2899\n",
      "Epoch 130/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.8875 - precision: 0.8148 - recall: 0.2657 \n",
      "Epoch 131/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.8872 - precision: 0.7625 - recall: 0.2947\n",
      "Epoch 132/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8879 - precision: 0.7756 - recall: 0.2923 \n",
      "Epoch 133/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8889 - precision: 0.7799 - recall: 0.2995\n",
      "Epoch 134/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.8916 - precision: 0.7939 - recall: 0.3164 \n",
      "Epoch 135/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.8892 - precision: 0.8000 - recall: 0.2899\n",
      "Epoch 136/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.8885 - precision: 0.8284 - recall: 0.2681 \n",
      "Epoch 137/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.8926 - precision: 0.8205 - recall: 0.3092\n",
      "Epoch 138/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.8885 - precision: 0.8099 - recall: 0.2778\n",
      "Epoch 139/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.8872 - precision: 0.7442 - recall: 0.3092\n",
      "Epoch 140/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.8841 - precision: 0.8151 - recall: 0.2343 \n",
      "Epoch 141/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8814 - precision: 0.7055 - recall: 0.2778 \n",
      "Epoch 142/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.8817 - precision: 0.7048 - recall: 0.2826 \n",
      "Epoch 143/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2699 - accuracy: 0.8848 - precision: 0.7278 - recall: 0.2971 \n",
      "Epoch 144/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8841 - precision: 0.7419 - recall: 0.2778\n",
      "Epoch 145/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.8776 - precision: 0.7745 - recall: 0.1908\n",
      "Epoch 146/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8858 - precision: 0.7564 - recall: 0.2850\n",
      "Epoch 147/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8882 - precision: 0.7806 - recall: 0.2923 \n",
      "Epoch 148/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.8827 - precision: 0.7290 - recall: 0.2729\n",
      "Epoch 149/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.8882 - precision: 0.7806 - recall: 0.2923\n",
      "Epoch 150/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2517 - accuracy: 0.8879 - precision: 0.7829 - recall: 0.2874\n",
      "Epoch 151/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8882 - precision: 0.7771 - recall: 0.2947\n",
      "Epoch 152/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2510 - accuracy: 0.8899 - precision: 0.7805 - recall: 0.3092 \n",
      "Epoch 153/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8940 - precision: 0.8514 - recall: 0.3043\n",
      "Epoch 154/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8926 - precision: 0.8247 - recall: 0.3068\n",
      "Epoch 155/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.8933 - precision: 0.8036 - recall: 0.3261\n",
      "Epoch 156/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8844 - precision: 0.8016 - recall: 0.2440\n",
      "Epoch 157/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.8882 - precision: 0.7605 - recall: 0.3068\n",
      "Epoch 158/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8848 - precision: 0.7550 - recall: 0.2754\n",
      "Epoch 159/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.8875 - precision: 0.7673 - recall: 0.2947 \n",
      "Epoch 160/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.8899 - precision: 0.7473 - recall: 0.3357\n",
      "Epoch 161/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.8933 - precision: 0.8148 - recall: 0.3188\n",
      "Epoch 162/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.8896 - precision: 0.7630 - recall: 0.3188 \n",
      "Epoch 163/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2568 - accuracy: 0.8913 - precision: 0.7526 - recall: 0.3454 \n",
      "Epoch 164/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8872 - precision: 0.7414 - recall: 0.3116 \n",
      "Epoch 165/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2496 - accuracy: 0.8899 - precision: 0.7500 - recall: 0.3333\n",
      "Epoch 166/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8906 - precision: 0.7831 - recall: 0.3140\n",
      "Epoch 167/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.8947 - precision: 0.8081 - recall: 0.3357\n",
      "Epoch 168/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8926 - precision: 0.8012 - recall: 0.3213\n",
      "Epoch 169/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.8903 - precision: 0.8000 - recall: 0.2995\n",
      "Epoch 170/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8944 - precision: 0.8523 - recall: 0.3068\n",
      "Epoch 171/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.8913 - precision: 0.7963 - recall: 0.3116\n",
      "Epoch 172/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8838 - precision: 0.7500 - recall: 0.2681\n",
      "Epoch 173/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8872 - precision: 0.8000 - recall: 0.2705\n",
      "Epoch 174/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.8872 - precision: 0.7471 - recall: 0.3068\n",
      "Epoch 175/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2475 - accuracy: 0.8916 - precision: 0.8345 - recall: 0.2923 \n",
      "Epoch 176/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.8916 - precision: 0.7904 - recall: 0.3188 \n",
      "Epoch 177/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2454 - accuracy: 0.8913 - precision: 0.7927 - recall: 0.3140\n",
      "Epoch 178/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8906 - precision: 0.7901 - recall: 0.3092\n",
      "Epoch 179/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.8855 - precision: 0.7548 - recall: 0.2826\n",
      "Epoch 180/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2596 - accuracy: 0.8879 - precision: 0.7622 - recall: 0.3019\n",
      "Epoch 181/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8865 - precision: 0.7808 - recall: 0.2754\n",
      "Epoch 182/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8875 - precision: 0.7852 - recall: 0.2826\n",
      "Epoch 183/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8882 - precision: 0.7843 - recall: 0.2899\n",
      "Epoch 184/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8858 - precision: 0.8390 - recall: 0.2391\n",
      "Epoch 185/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.8909 - precision: 0.7714 - recall: 0.3261\n",
      "Epoch 186/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.8831 - precision: 0.7687 - recall: 0.2488\n",
      "Epoch 187/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2591 - accuracy: 0.8903 - precision: 0.8000 - recall: 0.2995 \n",
      "Epoch 188/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.8862 - precision: 0.7647 - recall: 0.2826\n",
      "Epoch 189/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8851 - precision: 0.7600 - recall: 0.2754 \n",
      "Epoch 190/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.8892 - precision: 0.7557 - recall: 0.3213 \n",
      "Epoch 191/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.8882 - precision: 0.7881 - recall: 0.2874 \n",
      "Epoch 192/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.8906 - precision: 0.8133 - recall: 0.2947\n",
      "Epoch 193/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8889 - precision: 0.8248 - recall: 0.2729\n",
      "Epoch 194/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.8899 - precision: 0.8239 - recall: 0.2826\n",
      "Epoch 195/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2524 - accuracy: 0.8903 - precision: 0.8163 - recall: 0.2899\n",
      "Epoch 196/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.8899 - precision: 0.8026 - recall: 0.2947 \n",
      "Epoch 197/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.8889 - precision: 0.8560 - recall: 0.2585 \n",
      "Epoch 198/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.8875 - precision: 0.8195 - recall: 0.2633\n",
      "Epoch 199/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2530 - accuracy: 0.8923 - precision: 0.8462 - recall: 0.2923 \n",
      "Epoch 200/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8909 - precision: 0.8146 - recall: 0.2971\n",
      "Epoch 201/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8889 - precision: 0.7908 - recall: 0.2923\n",
      "Epoch 202/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8896 - precision: 0.8138 - recall: 0.2850\n",
      "Epoch 203/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8862 - precision: 0.8584 - recall: 0.2343\n",
      "Epoch 204/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8882 - precision: 0.8222 - recall: 0.2681\n",
      "Epoch 205/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.8944 - precision: 0.8723 - recall: 0.2971\n",
      "Epoch 206/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8814 - precision: 0.7724 - recall: 0.2295\n",
      "Epoch 207/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.8937 - precision: 0.8503 - recall: 0.3019\n",
      "Epoch 208/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8882 - precision: 0.8129 - recall: 0.2729 \n",
      "Epoch 209/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.8899 - precision: 0.8194 - recall: 0.2850\n",
      "Epoch 210/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.8858 - precision: 0.7985 - recall: 0.2585\n",
      "Epoch 211/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.8906 - precision: 0.8406 - recall: 0.2802\n",
      "Epoch 212/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.8885 - precision: 0.8333 - recall: 0.2657 \n",
      "Epoch 213/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8885 - precision: 0.8385 - recall: 0.2633\n",
      "Epoch 214/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8855 - precision: 0.8211 - recall: 0.2440 \n",
      "Epoch 215/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.8858 - precision: 0.7941 - recall: 0.2609\n",
      "Epoch 216/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.8896 - precision: 0.8227 - recall: 0.2802\n",
      "Epoch 217/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.8875 - precision: 0.8058 - recall: 0.2705\n",
      "Epoch 218/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2596 - accuracy: 0.8896 - precision: 0.7935 - recall: 0.2971\n",
      "Epoch 219/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.8896 - precision: 0.8013 - recall: 0.2923 \n",
      "Epoch 220/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.8920 - precision: 0.8500 - recall: 0.2874 \n",
      "Epoch 221/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2551 - accuracy: 0.8872 - precision: 0.8000 - recall: 0.2705 \n",
      "Epoch 222/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.8896 - precision: 0.8227 - recall: 0.2802\n",
      "Epoch 223/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8872 - precision: 0.8043 - recall: 0.2681 \n",
      "Epoch 224/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.8885 - precision: 0.8548 - recall: 0.2560\n",
      "Epoch 225/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8916 - precision: 0.8089 - recall: 0.3068\n",
      "Epoch 226/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8916 - precision: 0.8255 - recall: 0.2971\n",
      "Epoch 227/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2530 - accuracy: 0.8903 - precision: 0.8345 - recall: 0.2802\n",
      "Epoch 228/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.8930 - precision: 0.8389 - recall: 0.3019 \n",
      "Epoch 229/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2519 - accuracy: 0.8892 - precision: 0.8409 - recall: 0.2681\n",
      "Epoch 230/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8855 - precision: 0.8211 - recall: 0.2440\n",
      "Epoch 231/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.8930 - precision: 0.8686 - recall: 0.2874\n",
      "Epoch 232/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.8906 - precision: 0.8456 - recall: 0.2778\n",
      "Epoch 233/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.8855 - precision: 0.7970 - recall: 0.2560\n",
      "Epoch 234/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2463 - accuracy: 0.8920 - precision: 0.8603 - recall: 0.2826\n",
      "Epoch 235/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.8896 - precision: 0.8273 - recall: 0.2778\n",
      "Epoch 236/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8926 - precision: 0.8521 - recall: 0.2923\n",
      "Epoch 237/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.8903 - precision: 0.8207 - recall: 0.2874 \n",
      "Epoch 238/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8885 - precision: 0.8056 - recall: 0.2802\n",
      "Epoch 239/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8906 - precision: 0.8310 - recall: 0.2850\n",
      "Epoch 240/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.8872 - precision: 0.7917 - recall: 0.2754\n",
      "Epoch 241/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.8913 - precision: 0.8243 - recall: 0.2947  \n",
      "Epoch 242/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8872 - precision: 0.8443 - recall: 0.2488 \n",
      "Epoch 243/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.8899 - precision: 0.8108 - recall: 0.2899\n",
      "Epoch 244/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.8899 - precision: 0.8333 - recall: 0.2778\n",
      "Epoch 245/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.8933 - precision: 0.8643 - recall: 0.2923\n",
      "Epoch 246/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.8882 - precision: 0.8175 - recall: 0.2705  \n",
      "Epoch 247/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.8906 - precision: 0.8672 - recall: 0.2681\n",
      "Epoch 248/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8903 - precision: 0.8000 - recall: 0.2995\n",
      "Epoch 249/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.8906 - precision: 0.8406 - recall: 0.2802\n",
      "Epoch 250/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.8892 - precision: 0.8462 - recall: 0.2657\n",
      "Epoch 251/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.8903 - precision: 0.8974 - recall: 0.2536\n",
      "Epoch 252/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.8889 - precision: 0.8397 - recall: 0.2657\n",
      "Epoch 253/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2530 - accuracy: 0.8872 - precision: 0.8559 - recall: 0.2440\n",
      "Epoch 254/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.8913 - precision: 0.8429 - recall: 0.2850\n",
      "Epoch 255/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8851 - precision: 0.7955 - recall: 0.2536\n",
      "Epoch 256/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8909 - precision: 0.8417 - recall: 0.2826 \n",
      "Epoch 257/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8923 - precision: 0.8278 - recall: 0.3019\n",
      "Epoch 258/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.8916 - precision: 0.8345 - recall: 0.2923\n",
      "Epoch 259/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.8892 - precision: 0.8409 - recall: 0.2681\n",
      "Epoch 260/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.8885 - precision: 0.8014 - recall: 0.2826 \n",
      "Epoch 261/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8766 - precision: 0.8118 - recall: 0.1667 \n",
      "Epoch 262/300\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8896 - precision: 0.8273 - recall: 0.2778\n",
      "Epoch 263/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.8896 - precision: 0.8370 - recall: 0.2729\n",
      "Epoch 264/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.8920 - precision: 0.8551 - recall: 0.2850 \n",
      "Epoch 265/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8896 - precision: 0.8227 - recall: 0.2802\n",
      "Epoch 266/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.8855 - precision: 0.8211 - recall: 0.2440 \n",
      "Epoch 267/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.8865 - precision: 0.8796 - recall: 0.2295\n",
      "Epoch 268/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.8892 - precision: 0.8261 - recall: 0.2754\n",
      "Epoch 269/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.8916 - precision: 0.8440 - recall: 0.2874\n",
      "Epoch 270/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.8899 - precision: 0.8382 - recall: 0.2754 \n",
      "Epoch 271/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.8851 - precision: 0.8197 - recall: 0.2415\n",
      "Epoch 272/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.8906 - precision: 0.8561 - recall: 0.2729\n",
      "Epoch 273/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.8899 - precision: 0.8333 - recall: 0.2778 \n",
      "Epoch 274/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.8889 - precision: 0.8296 - recall: 0.2705\n",
      "Epoch 275/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.8926 - precision: 0.8521 - recall: 0.2923\n",
      "Epoch 276/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8875 - precision: 0.8195 - recall: 0.2633\n",
      "Epoch 277/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8926 - precision: 0.8846 - recall: 0.2778\n",
      "Epoch 278/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8814 - precision: 0.8384 - recall: 0.2005 \n",
      "Epoch 279/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.8913 - precision: 0.8243 - recall: 0.2947\n",
      "Epoch 280/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.8875 - precision: 0.8632 - recall: 0.2440 \n",
      "Epoch 281/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8875 - precision: 0.8696 - recall: 0.2415\n",
      "Epoch 282/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.8899 - precision: 0.8286 - recall: 0.2802\n",
      "Epoch 283/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8865 - precision: 0.8203 - recall: 0.2536 \n",
      "Epoch 284/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.8913 - precision: 0.8692 - recall: 0.2729\n",
      "Epoch 285/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.8851 - precision: 0.8362 - recall: 0.2343 \n",
      "Epoch 286/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2468 - accuracy: 0.8899 - precision: 0.8194 - recall: 0.2850\n",
      "Epoch 287/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.8930 - precision: 0.8686 - recall: 0.2874\n",
      "Epoch 288/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.8913 - precision: 0.8380 - recall: 0.2874 \n",
      "Epoch 289/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.8916 - precision: 0.8489 - recall: 0.2850\n",
      "Epoch 290/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8920 - precision: 0.8551 - recall: 0.2850 \n",
      "Epoch 291/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.8889 - precision: 0.8618 - recall: 0.2560\n",
      "Epoch 292/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.8923 - precision: 0.8722 - recall: 0.2802\n",
      "Epoch 293/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.8865 - precision: 0.7887 - recall: 0.2705\n",
      "Epoch 294/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8885 - precision: 0.8099 - recall: 0.2778 \n",
      "Epoch 295/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.8892 - precision: 0.8462 - recall: 0.2657\n",
      "Epoch 296/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.8868 - precision: 0.8029 - recall: 0.2657\n",
      "Epoch 297/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.8865 - precision: 0.8534 - recall: 0.2391 \n",
      "Epoch 298/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.8875 - precision: 0.7778 - recall: 0.2874\n",
      "Epoch 299/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8879 - precision: 0.7986 - recall: 0.2778\n",
      "Epoch 300/300\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.8903 - precision: 0.8496 - recall: 0.2729 \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d52eb6",
   "metadata": {},
   "source": [
    "### DNN implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77620a0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-479a192d89ce>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-479a192d89ce>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    self.w1 = self.w1 - self.lr * gr_w1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DenseNetwork:\n",
    "    \n",
    "    def __init__(self, X, y, m, n, lr):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.count_of_hidden_layers_neurons = m\n",
    "        self.output_size = n\n",
    "        self.lr = lr\n",
    "        self.w1 = np.random.randn((self.X.shape[1], self.m))\n",
    "        self.w2 = np.random.randn(( self.m, self.n))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(list(np.zeros_like(x)), list(x))\n",
    "    \n",
    "#     def softmax(self, x):\n",
    "#         return np.exp(x)/sum(np.exp(x))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return(1/(1 + np.exp(-x)))\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        X = np.insert(self.X, 0, 1, 1)\n",
    "        hidden_layers_input = self.relu(self.X @ self.w1)\n",
    "        hidden_layers_output = self.sigmoid(hidden_layers_input @ self.w2)\n",
    "               \n",
    "        return hidden_layers_output\n",
    "        \n",
    "    \n",
    "    \n",
    "    def backward_propagation(self):\n",
    "        gr_w1 = -1/(2*self.X.shape[1]) * (self.y - (self.X @ self.w)) * (self.sigmoid(self.X @ self.w))  @ (1-self.sigmoid(self.X @ self.w) @ self.w2 * self.X)\n",
    "        gr_w2 = -1/(2*self.X.shape[1]) * (self.y - (self.X @ self.w)) * (self.sigmoid(self.X @ self.w))  @ (1-self.sigmoid(self.X @ self.w)\n",
    "        self.w1 = self.w1 - self.lr * gr_w1                                                                \n",
    "        self.w2 = self.w2 - self.lr * gr_w2 \n",
    "                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea41b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7a5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249bf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ddbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
